{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "This is the first of two mandatory assignments which must be completed during the course. First some practical information:\n",
    "\n",
    "* When is the assignment due?: **23:59, Friday, August 24, 2018**\n",
    "* How do you grade the assignment?: You will **peergrade** each other as primary grading. \n",
    "* Must I hand-in as a group?: **yes**\n",
    "\n",
    "The assigment consist of one to three problems from each of the exercise sets you have solved so far (excluding Exercise Set 1). We've tried to select problems which are self contained, but it might be nessecary to solve some of the previous exercises in each set to fully answer the problems in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T20:29:59.252634Z",
     "start_time": "2018-08-22T20:29:55.024392Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems from exercise set 11\n",
    "\n",
    "> **_Hint:_** you can get data by running \n",
    "\n",
    "```python\n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac = 1, random_state = 3)\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "\n",
    "# A very simple deterministic test-train split \n",
    "Xtrain = X[:70]\n",
    "ytrain = y[:70]\n",
    "\n",
    "Xtest = X[70:]\n",
    "ytest = y[70:]\n",
    "```\n",
    "\n",
    "> The following code gives you the required functions to work with:\n",
    "\n",
    "```python\n",
    "def random_weights(location = 0.0, scale = 0.01, seed = 1):\n",
    "    # Init random number generator\n",
    "    rgen = np.random.RandomState(seed)\n",
    "    w = rgen.normal(loc=location, scale=scale, size= 1 + X.shape[1])    \n",
    "    return w\n",
    "\n",
    "def net_input(X, W): \n",
    "    return np.dot(X, W[1:]) + W[0]   # Linear product X'W + bias\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    linProd = net_input(X, W)\n",
    "    return np.where(linProd >= 0.0, 1, -1)    # 1(linProd > 0)\n",
    "```\n",
    "\n",
    "\n",
    "> **Ex. 11.1.5:** Write a function whichs loops over the training data (both X and y) using `zip`. For each row in the data, update the weights according to the perceptron rule (remember to update the bias in `w[0]`!). Set $\\eta = 0.1$.\n",
    ">\n",
    "> Make sure the loop stores the total number of prediction errors encountered underways in the loop by creating an int which is incremented whenever you update the weights. \n",
    ">\n",
    ">> _Hint:_ your function should return the updated weights, as well as the number of errors made by the perceptron.\n",
    ">\n",
    ">> _Hint:_ The following code block implements the function in _pseudo_code (it wont run, but serves to communicate the functionality).\n",
    ">> ```\n",
    ">> function f(X, y, W, eta):\n",
    ">>    set errors = 0\n",
    ">>\n",
    ">>    for each pair xi, yi in zip(X,y) do:\n",
    ">>        set update = eta * (yi - predict(xi, W))\n",
    ">>        set W[1:] = W[1:] + update * xi\n",
    ">>        set W[0] = W[0] + update\n",
    ">>        set errors = errors + int(update != 0) \n",
    ">>\n",
    ">>    return W, errors\n",
    ">> ```\n",
    ">\n",
    "> *Bonus:* If you completed the previous bonus assignment, calculate the accuracy on training data using the updated weights. Any progress yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T09:27:45.183300Z",
     "start_time": "2018-08-24T09:27:23.701251Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T16:32:24.304802Z",
     "start_time": "2018-08-22T16:32:24.125792Z"
    }
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac = 1, random_state = 3)\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "\n",
    "# A very simple deterministic test-train split \n",
    "Xtrain = X[:70]\n",
    "ytrain = y[:70]\n",
    "\n",
    "Xtest = X[70:]\n",
    "ytest = y[70:]\n",
    "\n",
    "#The following code gives you the required functions to work with:\n",
    "\n",
    "def random_weights(location = 0.0, scale = 0.01, seed = 1):\n",
    "    # Init random number generator\n",
    "    rgen = np.random.RandomState(seed)\n",
    "    w = rgen.normal(loc=location, scale=scale, size= 1 + X.shape[1])    \n",
    "    return w\n",
    "\n",
    "def net_input(X, W): \n",
    "    return np.dot(X, W[1:]) + W[0]   # Linear product X'W + bias\n",
    "\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    linProd = net_input(X, W)\n",
    "    return np.where(linProd >= 0.0, 1, -1)    # 1(linProd > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T16:32:24.355805Z",
     "start_time": "2018-08-22T16:32:24.316803Z"
    }
   },
   "outputs": [],
   "source": [
    "W = random_weights()\n",
    "z = net_input(Xtrain, W)\n",
    "y_hat = predict(Xtrain,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T16:32:24.399807Z",
     "start_time": "2018-08-22T16:32:24.364805Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 11.1.5]\n",
    "def f(X, y, W, eta):\n",
    "    errors = 0\n",
    "    \n",
    "    for xi, yi in zip(Xtrain, y):\n",
    "        bias = eta * (yi - predict(xi, W))\n",
    "        W[1:] = W[1:] + bias * xi\n",
    "        W[0] = W[0] + bias\n",
    "        errors = errors + int(bias != 0)\n",
    "    \n",
    "    return W, errors\n",
    "\n",
    "eta = 0.1\n",
    "f(Xtrain, y, W, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.6:** Write a function which repeat the updating procedure you constructed in 11.1.5 for `n_iter` times by packing the whole thing in an outer loop. Make sure you store the number of errors in each iteration in a list. \n",
    ">\n",
    "> Plot the total errors after each iteration in a graph.\n",
    ">\n",
    ">> _Hint:_ Make sure you dont reset the weights after each iteration.\n",
    ">\n",
    ">> _Hint:_ Once again some pseudocode:\n",
    ">> ```\n",
    ">> function g(X, y, n_iter):\n",
    ">>     set eta = 0.1\n",
    ">>     set weights = random_weights()\n",
    ">>     set errorseq = list()\n",
    ">>\n",
    ">>     for each _ in range(n_iter):\n",
    ">>         weights, e = f(X, y, W, eta) \n",
    ">>         errorseq.append(e)\n",
    ">>\n",
    ">>     return weights, errorseq\n",
    ">> ```\n",
    ">\n",
    ">> _Bonus:_ Wrap the code in a function called Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 11.1.6]\n",
    "def Perceptron(X, y, n_iter):\n",
    "    eta = 0.1\n",
    "    W = random_weights()\n",
    "    errorseq = list()\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        w, e = f(X,y,W,eta)\n",
    "        errorseq.append(e)\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Number of errors\")\n",
    "    return print(\"Weight vector: \" + str(W)), \\\n",
    "            plt.plot(errorseq), \\\n",
    "            print(\"First 10 errors: \" + str(errorseq[:10])), \\\n",
    "            print(\"Last 10 errors: \" + str(errorseq[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems from exercise set 12\n",
    ">Get the required data by running \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.0:** Load the california housing data with scikit-learn using the code above. Inspect the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T20:30:03.108854Z",
     "start_time": "2018-08-22T20:30:03.025849Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.0]\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? \n",
    ">\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    ">\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    ">\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:18:05.932845Z",
     "start_time": "2018-08-22T21:18:05.886045Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.1]\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#The third degree interactions are defined in X_pol based on the TRAINING dataset\n",
    "X_pol = PolynomialFeatures(degree = 3, interaction_only=True, include_bias=False).fit_transform(X_train)\n",
    "# and contain 10320 variables (holy moly..)\n",
    "print(len(X_pol))\n",
    "\n",
    "#The interaction features are rescaled - we only rescale the TRAIN dataset.\n",
    "X_train_rescale = StandardScaler(X_train)\n",
    "print(X_train_rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.2**: Estimate the Lasso model on the train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the test and train data. \n",
    ">\n",
    ">> *Hint*: use `logspace` in numpy to create the range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T21:18:04.513242Z",
     "start_time": "2018-08-22T21:18:04.497642Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.2]\n",
    "\n",
    "lambdas = np.logspace(-4, 4, 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.3**: Make a plot with on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems from exercise set 13\n",
    "\n",
    "> **Ex. 13.1.3:**\n",
    "Run a Lasso regression using the Pipeline from `Ex 13.1.2`. In the outer loop searching through the lambdas specified below. \n",
    "In the inner loop make 5 fold cross validation on the selected model and store the average MSE for each fold. Which lambda gives the lowest test MSE?\n",
    ">\n",
    "> ```python \n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "```\n",
    ">\n",
    ">> *Hint:* `KFold` in `sklearn.model_selection` may be useful.\n",
    ">\n",
    "> This code will give you the required data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 13.1.3]\n",
    "#split data into development and test \n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\n",
    "\n",
    "#split development data into train & validation \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=1)\n",
    "\n",
    "X_train_index = X_train.index\n",
    "X_val_index = X_val.index\n",
    "#y_train_index = y_train.index\n",
    "#y_val_index = y_val.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=5) #splits the data into 5 equally large splits \n",
    "lambdas = np.logspace(-4, 4, 12)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),\n",
    "                                 StandardScaler(),\n",
    "                                 Lasso(alpha=lambda_, random_state=1))    \n",
    "    mseCV_ = []\n",
    "    \n",
    "    for train_index, val_index in kfolds.split(X_dev, y_dev):\n",
    "        \n",
    "        X_train,  X_val = X_dev.loc[X_train_index], X_dev.loc[X_val_index]\n",
    "        y_train, y_val = y_dev.loc[X_train_index], y_dev.loc[y_val_index]\n",
    "        \n",
    "        #X_train, y_train, = X_dev[train_idx], y_dev[train_idx]\n",
    "        #X_val, y_val = X_dev[val_idx], y_dev[val_idx] \n",
    "\n",
    "        pipe_lassoCV.fit(X_train, y_train)\n",
    "        \n",
    "        mseCV_.append(mse(pipe_lassoCV.predict(X_val), y_val)) \n",
    "        hyperparam_perform = pd.Series (mseCV_, index=lamdas)\n",
    "        #optimal = pd.Series(mseCV_, index=lambdas).nsmallest(1)\n",
    "    \n",
    "    mseCV.append(mseCV_)\n",
    "    \n",
    "#something's wrong with the indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems from exercise set 14\n",
    "\n",
    "> **Ex. 14.1.3**: Train a decision tree classifier on **all** of the titanic data.\n",
    ">\n",
    ">* If your feature matrix is `X` and your target array is `y` you can do this by instantiating a model like:\n",
    ">\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X, y)  # <--- This is the training/fitting/learning step\n",
    ">       \n",
    ">Write four functions that counts the number of ..\n",
    " - true positives where we call the function `TP`;\n",
    " - true negatives where we call the function `TN`;\n",
    " - false positives where we call the function `FP`;\n",
    " - false negatives where we call the function `FN`.\n",
    "\n",
    "> All of these functions should take three arguments, the actual y column, the actual X column and a fittedModel object (e.g. `fittedModel = DecisionTreeClassifier.fit(X,y)`)\n",
    ">\n",
    ">> _Hint 1:_ use the function `np.where` to compare y and the predicted values. For example `y + prediction == 2` is true only for the True Positives.\n",
    ">\n",
    ">> _Hint 2:_ You can check if your result is correct by summing all four functions and checking that the result equals to the number of observations. \n",
    ">\n",
    "> Get the data by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T09:27:45.613300Z",
     "start_time": "2018-08-24T09:27:45.193300Z"
    }
   },
   "outputs": [],
   "source": [
    "rawdata = sns.load_dataset('titanic').sample(frac=1, random_state = 3)\n",
    "\n",
    "# Feature creation\n",
    "rawdata['male'] = (rawdata['sex'] == 'male').astype(int)\n",
    "rawdata['alone'] = rawdata['alone'].astype(int)\n",
    "rawdata['adult_male'] = rawdata['adult_male'].astype(int)\n",
    "\n",
    "\n",
    "# The following piece of code (in multiple lines) generates \n",
    "# dummies for all of the categorical variables. \n",
    "data = pd.get_dummies(rawdata, \n",
    "                      columns = ['class', 'sibsp', 'parch', 'deck'], \n",
    "                      drop_first=True)\\\n",
    "         .drop(['pclass', 'sex', 'embarked', 'who', 'embark_town', 'alive'], axis = 1)\n",
    "\n",
    "\n",
    "data = data.dropna().reset_index()\n",
    "\n",
    "X = data.iloc[:,2:] #dataframe including all columns from data except 'survived'\n",
    "y = data.iloc[:,1] #list of column = 'survived' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex 14.1.3] - we provide two answers to this question: \n",
    "# 1) using the confusion matrix (because we find it simpler) & 2) writing up the four functions as expressed in the exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T09:28:02.107335Z",
     "start_time": "2018-08-24T09:28:00.256332Z"
    }
   },
   "outputs": [],
   "source": [
    "#confusion matrix \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "fittedModel = model.fit(X, y) \n",
    "pred = fittedModel.predict(X)\n",
    "\n",
    "cm=confusion_matrix(y, pred)\n",
    "print(cm)\n",
    "\n",
    "TP=cm[1][1] #true positives variable\n",
    "TN=cm[0][0] #true negatives variable\n",
    "FN=cm[0][1] #false negatives variable\n",
    "FP=cm[1][0] #false positives variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T09:28:28.499385Z",
     "start_time": "2018-08-24T09:28:03.057336Z"
    }
   },
   "outputs": [],
   "source": [
    "#four functions\n",
    "\n",
    "# 1) true positives\n",
    "def true_positives(y, X, fittedModel):\n",
    "    for x in y:\n",
    "        model = DecisionTreeClassifier()\n",
    "        fittedModel = model.fit(X, y)\n",
    "        y_hat = fittedModel.predict(X)\n",
    "        compare = np.where(y + y_hat == 2, 1, 0)\n",
    "        number = y_hat + compare\n",
    "    \n",
    "    return number\n",
    "\n",
    "print('true positives =', list(true_positives(y, X, fittedModel)).count(2))\n",
    "\n",
    "\n",
    "# 2) false positives \n",
    "\n",
    "def false_positives(y, X, fittedModel):\n",
    "    for x in y:\n",
    "        model = DecisionTreeClassifier()\n",
    "        fittedModel = model.fit(X, y)\n",
    "        y_hat = fittedModel.predict(X)\n",
    "        compare = np.where(y + y_hat == 0, 1, 0)\n",
    "        number = y_hat + compare\n",
    "    \n",
    "    return number\n",
    "\n",
    "print('false positives =', list(false_positives(y, X, fittedModel)).count(0))\n",
    "\n",
    "\n",
    "# 3) true negatives\n",
    "\n",
    "def true_negatives(y, X, fittedModel):\n",
    "    for x in y:\n",
    "        model = DecisionTreeClassifier()\n",
    "        fittedModel = model.fit(X, y)\n",
    "        y_hat = fittedModel.predict(X)\n",
    "        compare = np.where(y + y_hat == 1, 0, 1)\n",
    "        number = y_hat + compare\n",
    "    \n",
    "    return number\n",
    "\n",
    "print('true negatives =', list(true_negatives(y, X, fittedModel)).count(1))\n",
    "\n",
    "\n",
    "# 4) false negatives (FN)\n",
    "\n",
    "def false_negatives(y, X, fittedModel):\n",
    "    for x in y:\n",
    "        model = DecisionTreeClassifier()\n",
    "        fittedModel = model.fit(X, y)\n",
    "        y_hat = fittedModel.predict(X)\n",
    "        compare = np.where(y + y_hat == 2, 0, 1)\n",
    "        number = y_hat + compare\n",
    "    \n",
    "    return number    \n",
    "\n",
    "print('false negatives =', list(false_negatives(y, X, fittedModel)).count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.4:** Combine the four functions you defined above to write your own accuracy function, which calculates\n",
    "$$\n",
    "ACC = \\frac{TP + TN}{TP+ TN + FP + FN}\n",
    "$$\n",
    ">\n",
    "> Test the accuracy of your model using the `A` function. Report the accuracy of your model on the same data that you trained the model on.\n",
    ">\n",
    ">> _Note:_ The reason we want to split the calculation of accuracy into these four components, is that we can then easily calculate other scores, such as the _precision, recall and f1_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T09:28:28.529385Z",
     "start_time": "2018-08-24T09:28:28.499385Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex 14.1.4]\n",
    "\n",
    "A=(TP+TN)/(TP+TN+FP+FN) #accuracy score \n",
    "P=(TP/(TP+FP)) #precision score\n",
    "R=TP/(TP+FN) #recall score\n",
    "F1=2*P*R/(P+R) #f1 score \n",
    "print('accuracy =', A, 'precision =', P, 'recall =', R, 'f1 =', F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems from exercise set 15 \n",
    "\n",
    "> **Ex. 15.1.1:** \n",
    "Import the Counter object from the builtin package collections (Hint1). This is essentially a dictionary designed for keeping counts, same syntax, but extra functionality. We don't have to initialize each key. We can write: \n",
    "\n",
    "```python\n",
    "c = Counter()\n",
    "# then we can do this\n",
    "c['hej']+=1\n",
    "# without first defining c['hej'] = 0\n",
    "```\n",
    "\n",
    "\n",
    ">* Initialize a Counter object and assign it to the variable `dc` (document count).\n",
    ">* Define a list named `text_counts`. In this container we will store each document after we have converted it to counts of tokens.\n",
    ">* Run through all tokenized texts and\n",
    "    * initialize a Counter object with the tokenized text as input, assign this object to a variable `c_t`. >This will now contain a count of each token in the document. Append `c_t` to our list `text_counts`.\n",
    "    * run though each key in the `c_t` and increment the document count variable `dc` by one. (Hint2)\n",
    "\n",
    "(hint1: from ... import ...)\n",
    "\n",
    "(hint2: dc[token]+=1)\n",
    "\n",
    "> Run the following code to get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv')\n",
    "import re\n",
    "token_re = re.compile('\\w+')\n",
    "\n",
    "tokenized = df.reviewBody.apply(nltk.word_tokenize)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 15.1.1]\n",
    "from collections import Counter\n",
    "dc = Counter()\n",
    "text_counts = []\n",
    "\n",
    "for text in tokenized:\n",
    "    c_t = Counter(text)\n",
    "    text_counts.append(c_t)\n",
    "    \n",
    "    for token in c_t:\n",
    "        dc[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's inspect it:\n",
    "print(text_counts[0:2])\n",
    "dc\n",
    "#Looks' good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.1.2:** \n",
    "Now we define the the inverse document frequency variable `idf` as a dictionary with the tokens as keys and idf weights as values. We do this by running through both the token and the value (document count) in the `dc` variable and calculate the ratio between number documents and the token document counts. \n",
    "\n",
    ">Use the `np.log` function for the log transform.\n",
    "\n",
    ">We can iterate through this using the `.items()` syntax we know from the dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 15.1.2]\n",
    "import numpy as np\n",
    "n_docs = len(tokenized)\n",
    "\n",
    "#Using a loop:\n",
    "# idf = {}\n",
    "# for k, v in dc.items():\n",
    "#     idf[k] = np.log(n_docs / value)\n",
    "\n",
    "#Using an smooth oneliner:\n",
    "idf = {key:np.log(n_docs / val) for key,val in dc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's inspect the Inverse Document Frequency:\n",
    "idf\n",
    "#Look's good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.1.3:** \n",
    "Now we weight the term frequency in each document with the idf value of each token. Here we used our `text_counts` variable that almost holds the frequency, we just need to divide by the number of tokens in the document. \n",
    "Define a list container: `tfidf_docs`. \n",
    "\n",
    "FIRST LOOP: For each counter in the text_count container:\n",
    "    * define the variable `doc_n` as sum of all values in the counter - `.values()` .\n",
    "    * define a dictionary named `tfidf`.\n",
    "    * SECOND LOOP: run through all tokens, and their counts by using the `.items()` method of the counter.\n",
    "        * define a value tf as the ratio between the count and the sum.\n",
    "        * now weight this value with the idf weight found by calling the idf variable with the token as key.\n",
    "        * assign this weighed term frequency to the tfidf[token].\n",
    "    * Once outside the second loop. Append the tfidf dictionary to the tfidf_docs list container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 15.1.3]\n",
    "tfidf_docs = list()\n",
    "for object in text_counts:\n",
    "    doc_n = sum(object.values())\n",
    "    tfidf = {}\n",
    "    \n",
    "    for token, counter in dc.items():\n",
    "        tf = counter / doc_n\n",
    "        tfidf[token] = tf * idf[token] \n",
    "    \n",
    "    tfidf_docs.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's inspect the Term Frequency - Inverse Document Frequency:\n",
    "tfidf_docs[0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems from exercise set 16\n",
    "\n",
    "\n",
    "> **Ex. 16.2.5:** You should now implement it by doing the following:\n",
    "* Define a maximum number of iterations`max_iter` to 15.\n",
    "* Use the `initialize_clusters` function to define a variable `centroids`.\n",
    "* make a `for` loop from 0 to max_iter where you: \n",
    "    * copy the current cluster centroids to a new variable: old_centroids. This will be used for checking convergence after the maximization step.\n",
    "    * define the `cluster_assignment`  by running the `maximize` function\n",
    "    * define a new (i.e. overwrite) `centroids` variable by running the `update_expectation` function.\n",
    "    * finally check if old_centroids is equal to new_centroids, using the np.array_equal() function. If they are: break.\n",
    "\n",
    "Make sure that it works and wrap it around a function `fit_transform()` that takes the data `X` as input, and the number of clusters `k` plus the maximum number of iterations `max_iter`. It should return the cluster assignments and the cluster centroids. \n",
    "\n",
    "\n",
    "> The code below will give you the dataset, as well as three functions you need to solve the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "df = sns.load_dataset('iris')\n",
    "X = df[['sepal_length','sepal_width','petal_length','petal_width']].values\n",
    "\n",
    "\n",
    "def initialize_clusters(k,X):\n",
    "    idx = random.sample(range(len(X)),k)\n",
    "    centroids = X[idx]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def maximize(centroids,X):\n",
    "    dist_matrix = pairwise_distances(centroids,X)\n",
    "    cluster_assignment = dist_matrix.T.argsort(axis=1)[:,0]\n",
    "    return cluster_assignment\n",
    "\n",
    "\n",
    "def update_expectation(k,X,cluster_assignment):\n",
    "    new_centroids = np.zeros((k,len(X[0])))\n",
    "    for i in range(k):\n",
    "        subset = X[cluster_assignment==i]\n",
    "        new_centroids[i] = subset.mean(axis=0)\n",
    "    return new_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 16.2.5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
